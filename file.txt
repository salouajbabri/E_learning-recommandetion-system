%%configure -f
{
    "conf":
    {
        //# Compatibility with datetime < 1900-01-01
        "spark.sql.legacy.parquet.datetimeRebaseModeInRead": "CORRECTED",
        "spark.sql.legacy.parquet.int96RebaseModeInRead": "CORRECTED",
        "spark.sql.parquet.int96RebaseModeInWrite": "CORRECTED",
        "spark.sql.parquet.datetimeRebaseModeInWrite": "CORRECTED",
        //# Compatibility with decimal columns in parquet
        "spark.sql.parquet.enableVectorizedReader": "false",
        //# Enables time travel up to 90 days in the past and prevent VACUUM from deleting files in this period
        "spark.databricks.delta.properties.defaults.logRetentionDuration": "interval 90 days",
        "spark.databricks.delta.properaties.defaults.deletedFileRetentionDuration": "interval 90 days",
        //# Optimize write feature (better read performance, slightly worse write performance)
        "spark.microsoft.delta.optimizeWrite.enabled": "true",
        "spark.sql.sources.partitionOverwriteMode": "DYNAMIC",
        "spark.databricks.delta.schema.autoMerge.enabled": "true"
    }
}
import pyspark.sql.functions as F
from pyspark.sql.window import Window
from pyspark.sql.types import StringType
import ast
from delta.tables import *

# Source parameters
source_path = ""
source_zone = ""
source_tables = ""
#source_path="RAW/DATASOURCE=EPV2_BI/"
#source_zone="azblb01raw@azsane08wedevraw01"
#source_tables = "[{\"schema_name\":\"DWH\",\"table_name\":\"TB_FactCertification\",\"is_incremental_load\":true,\"last_modified_column\":\"ETL_DTM_CreatedAt\",\"start_date\":\"2024-01-01T00:00:00+00:00\",\"primary_keys_list\":\"NUM_ProjectKey;NUM_CertificationKey\",\"partition_keys_list\":\"NUM_ProjectKey;NUM_CutOffKey\"}]"


# Destination parameters
destination_database = ""
destination_path = ""
destination_zone = ""
#destination_database="spot_epv2_bi"
#destination_path="CLEANSED/DATASOURCE=EPV2_BI/"
#destination_zone="azblb01cleansed@azsane08wedevcleansed01"
def convert_string_to_dict(encoded_string):
    """convert string encoded to dict

    Args:
        encoded_string (String): the string encodede
   
    Returns:
        Dict: return the encoded string to dict format
    """
    formated_string=encoded_string.replace("true","True")
    formated_string=formated_string.replace("false","False")
    formated_string=formated_string.replace("null","None")
    formated_string=ast.literal_eval(formated_string)
    return formated_string


def get_latest_partition(zone, path, folder):
    """Get the latest partition of the table
 
    Args:
        zone (String): the destination zone
        path (String): the path of the destination
        folder (String): the name of the folder
        incremental (Boolean): Is the table load in incremental
   
    Returns:
        String: the latest partition_date if exists, None otherwise
    """
    try:
        df=spark.read.load(f"abfss://{zone}.dfs.core.windows.net/{path}{folder}/", format='delta')
        last_update=df.select(F.max("partition_date").alias("last_update")).first()[0]
        return last_update
    except:
        return None


def generate_merge_condition(pk_cols):
    """Generate the merge condition
 
    Args:
        pk_cols (list): list of the columns generating the PK
   
    Returns:
        String: the condition query for the merge
    """
    condition = f"source.{pk_cols[0]} = destination.{pk_cols[0]}"

    for col in pk_cols[1:]: 
        condition += f" AND source.{col} = destination.{col}"
    
    print(f"condition={condition}")
    return condition
# Get list tables from catalog
source_tables=convert_string_to_dict(source_tables)

# Create spot DB
spark.sql(f"CREATE DATABASE IF NOT EXISTS {destination_database}")

# For each tables
for row in source_tables:
    print(f"{row['schema_name']}.{row['table_name']}")
    folder=f"{row['schema_name']}.{row['table_name']}"
    partition_keys=row['partition_keys_list'].split(";")
    primary_keys=row['primary_keys_list'].split(";")

    # get the latest partition on cleansed
    latest_partition=get_latest_partition(destination_zone, destination_path, folder)
    print(f"==>latest_partition={latest_partition}")

    # read by merging the schema
    try:
        df = (
            spark
            .read
            .option("mergeSchema", "true")
            .load(f'abfss://{source_zone}.dfs.core.windows.net/{source_path}{folder}', format='parquet')
        )

        if(latest_partition != None):
            df = df.filter(F.col("partition_date")>=latest_partition)
    except:
        print(f'the folder abfss://{source_zone}.dfs.core.windows.net/{source_path}{folder} doesnt exist')
        continue
    
    # keep the latest records
    df = (
        df
        .withColumn("row_number",
        F.row_number().over(Window.partitionBy(*primary_keys).orderBy(F.col("partition_date").desc(),F.col(f"{row['last_modified_column']}").desc())))
        .filter(F.col("row_number")==1)
        .drop("row_number")
    )

    # convert partition_date to string
    df = df.withColumn("partition_date", F.col("partition_date").cast(StringType()))

    if (df.count()!=0):
        try:
            df_delta = DeltaTable.forPath(spark, f"abfss://{destination_zone}.dfs.core.windows.net/{destination_path}{folder}/")
            (
                df_delta
                .alias('destination')
                .merge(df.alias('source'), generate_merge_condition(primary_keys))
                .whenMatchedUpdateAll()
                .whenNotMatchedInsertAll()
                .execute()
            )
        except:
            (
            df
            .write
            .option("mergeSchema", "true")
            .format("delta")
            .mode("overwrite")
            .partitionBy("partition_date", *partition_keys)
            .save(f"abfss://{destination_zone}.dfs.core.windows.net/{destination_path}{folder}/")
            )

    spark.sql(f"DROP TABLE IF EXISTS {destination_database}.{row['schema_name']}_{row['table_name']}")
    spark.sql(f"CREATE TABLE IF NOT EXISTS {destination_database}.{row['schema_name']}_{row['table_name']} USING DELTA LOCATION 'abfss://{destination_zone}.dfs.core.windows.net/{destination_path}{folder}'")
